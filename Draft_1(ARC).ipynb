{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsEoap8h2kdfgCWwXqkQjh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bwowejk/AGI-research/blob/main/Draft_1(ARC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip installs\n",
        "# Initial setup\n",
        "%%capture\n",
        "!pip install -U transformers accelerate bitsandbytes\n",
        "!pip install arckit\n"
      ],
      "metadata": {
        "id": "2PufPipDZbY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4y6_AV9M9O2"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import arckit\n",
        "import json\n",
        "from pathlib import Path\n",
        "import re\n",
        "import textwrap\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load ARC tasks and output prompts with tasks\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class ARCTasksHelper():\n",
        "  def __init__(self):\n",
        "    self.train_set = None\n",
        "    self.test_set = None\n",
        "\n",
        "\n",
        "  #loading arc tasks\n",
        "  def _load_tasks(self):\n",
        "    self.train_set, self.test_set = arckit.load_data(\"arcagi\")\n",
        "    print(\"ARC tasks loaded successfully\")\n",
        "\n",
        "  #construct a prompt to feed to the model\n",
        "  def _build_arc_prompt_from_arckit(self, task):\n",
        "    \"\"\"\n",
        "    Build an ARC prompt from an arckit.Task object,\n",
        "    using its .to_dict() representation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Use the dictionary form (matches official ARC JSON)\n",
        "    data = task.to_dict()\n",
        "\n",
        "    train_pairs = [(pair[\"input\"], pair[\"output\"]) for pair in data[\"train\"]]\n",
        "    test_inputs = [pair[\"input\"] for pair in data[\"test\"]]\n",
        "\n",
        "    header = \"\"\"\n",
        "You are an AI assistant specialized in solving Abstract Reasoning Corpus (ARC-AGI) tasks by\n",
        "generating Python code.\n",
        "Your goal is to analyze input-output grid pairs. The outputs were produced by applying a\n",
        "transformation rule to the inputs. Implement the transformation rules as a Python function.\n",
        "You should only write the implemented transformation in code.\n",
        "\n",
        "You must write code in triple backticks (```python and then ```). You must write a function\n",
        "called `transform` which takes a single argument, the input grid as `list[list[int]]`, and\n",
        "returns the transformed grid (also as `list[list[int]]`).\n",
        "You should make sure that you implement a version of the transformation which works in general\n",
        "(at least for all given input-output pairs and test input pairs).\n",
        "The number in the input grid can be mapped to the following colors: 0:Black; 1:Blue; 2:Red; 3:\n",
        "Green; 4:Yellow; 5:Grey; 6:Pink; 7:Orange; 8:Purple; 9:Brown\n",
        "\n",
        "Now, solve the following ARC-AGI task:\n",
        "    \"\"\"\n",
        "\n",
        "    def format_grid(grid):\n",
        "        rows = [\"[\" + \" \".join(str(x) for x in row) + \"]\" for row in grid]\n",
        "        return f\"[\" + \"\\n\".join(rows) + \"]\"\n",
        "\n",
        "    parts = []\n",
        "\n",
        "    # TRAIN\n",
        "    for i, (inp_grid, out_grid) in enumerate(train_pairs, start=1):\n",
        "        parts.append(\n",
        "f\"\"\"\n",
        "## Input {i} (grid shape: {len(inp_grid)} by {len(inp_grid[0])}):\n",
        "{format_grid(inp_grid)}\n",
        "\n",
        "## Output {i} (grid shape: {len(out_grid)} by {len(out_grid[0])}):\n",
        "{format_grid(out_grid)}\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "    # TEST\n",
        "    for j, inp_grid in enumerate(test_inputs, start=1):\n",
        "        parts.append(\n",
        "f\"\"\"\n",
        "## Test Input {j} (grid shape: {len(inp_grid)} by {len(inp_grid[0])}):\n",
        "{format_grid(inp_grid)}\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "    return header + \"\\n\".join(parts)\n",
        "\n",
        "\n",
        "  #generate prompts for tasks\n",
        "  def generate_task_dataset_prompts(self):\n",
        "\n",
        "    self._load_tasks()\n",
        "    train_prompts = []\n",
        "    test_prompts = []\n",
        "\n",
        "    for task in self.train_set:\n",
        "      prompt = self._build_arc_prompt_from_arckit(task)\n",
        "      train_prompts.append(prompt)\n",
        "\n",
        "    for task in self.test_set:\n",
        "      prompt = self._build_arc_prompt_from_arckit(task)\n",
        "      test_prompts.append(prompt)\n",
        "\n",
        "    print(\"\\nPrompts successfully generated\")\n",
        "    return train_prompts, test_prompts\n",
        "\n",
        "\n",
        "  #AFTER GETTING MODEL RESPONSE FUNCTIONS\n",
        "\n",
        "  #obtain the function in backticks from the text as string\n",
        "\n",
        "\n",
        "  def extract_python_function(self, text):\n",
        "      \"\"\"\n",
        "      Extracts the first Python function from text (from 'def' to 'return').\n",
        "      Returns a single function string or None if not found.\n",
        "      \"\"\"\n",
        "      # Match code blocks with various language tags\n",
        "      pattern = r\"```(?:python|py|notebook-python)?\\s*(.*?)```\"\n",
        "      blocks = re.findall(pattern, text, flags=re.DOTALL)\n",
        "\n",
        "      # If code blocks found, use them; otherwise use raw text\n",
        "      code_text = \"\\n\\n\".join(blocks) if blocks else text\n",
        "\n",
        "      # Extract first function from 'def' to 'return' statement\n",
        "      func_pattern = r\"(def\\s+\\w+\\s*\\([^)]*\\):.*?return\\s+[^\\n]+)\"\n",
        "      match = re.search(func_pattern, code_text, flags=re.DOTALL)\n",
        "\n",
        "      if match:\n",
        "          return match.group(1).strip()\n",
        "\n",
        "      # Fallback: if no return statement, try to get function with indentation logic\n",
        "      func_pattern_no_return = r\"(def\\s+\\w+\\s*\\([^)]*\\):(?:.*?)(?=\\ndef\\s+\\w+|\\Z))\"\n",
        "      match = re.search(func_pattern_no_return, code_text, flags=re.DOTALL)\n",
        "\n",
        "      if match:\n",
        "          return match.group(1).strip()\n",
        "\n",
        "      return None\n",
        "\n",
        "  #obtain the functions in backticks from the text as string\n",
        "  def extract_python_functions(self, text):\n",
        "    \"\"\"\n",
        "    Extracts individual Python functions from text.\n",
        "    Returns a list of function strings (from 'def' to 'return').\n",
        "    \"\"\"\n",
        "    # First, extract code from markdown code blocks if present\n",
        "    code_block_pattern = r\"```(?:python|py|notebook-python)?\\s*(.*?)```\"\n",
        "    code_blocks = re.findall(code_block_pattern, text, flags=re.DOTALL)\n",
        "\n",
        "    # If code blocks found, use them; otherwise use the raw text\n",
        "    code_text = \"\\n\\n\".join(code_blocks) if code_blocks else text\n",
        "\n",
        "    # Pattern to match function definitions from 'def' to 'return'\n",
        "    # This captures the function name, parameters, body, and the return statement\n",
        "    func_pattern = r\"(def\\s+\\w+\\s*\\([^)]*\\):.*?return\\s+[^\\n]+)\"\n",
        "\n",
        "    functions = re.findall(func_pattern, code_text, flags=re.DOTALL)\n",
        "\n",
        "    return [func.strip() for func in functions]\n",
        "\n",
        "\n",
        "  #evaluate the python in string on a grid\n",
        "  def run_candidate_func(self, candidate_str, input_grid):\n",
        "    namespace = {}\n",
        "    try:\n",
        "        # Convert string to callable function\n",
        "        exec(candidate_str, namespace)\n",
        "        candidate_func = namespace.get(\"transform\")\n",
        "\n",
        "        if candidate_func is None:\n",
        "            # Function not defined\n",
        "            return None\n",
        "\n",
        "        # Execute function\n",
        "        output = candidate_func(input_grid)\n",
        "        return output\n",
        "    except Exception as e:\n",
        "        # Any error is treated as failure\n",
        "        return None\n",
        "\n",
        "\n",
        "  #get scores for a given function\n",
        "  def score_candidate_on_train_pair(self, candidate_str, train_pairs):\n",
        "\n",
        "    total_cells = 0\n",
        "    matched_cells = 0\n",
        "\n",
        "    for input_grid, expected_output in train_pairs:\n",
        "        output = self.run_candidate_func(candidate_str, input_grid)\n",
        "        if output is None:\n",
        "            # Function failed to execute, score 0 for this example\n",
        "            continue\n",
        "\n",
        "        # Count matches per cell\n",
        "        for row_out, row_expected in zip(output, expected_output):\n",
        "            for val_out, val_expected in zip(row_out, row_expected):\n",
        "                total_cells += 1\n",
        "                if val_out == val_expected:\n",
        "                    matched_cells += 1\n",
        "\n",
        "    if total_cells == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return matched_cells / total_cells\n",
        "\n",
        "    #evaluate a single candidate function on all train and test grids, return dict\n",
        "  def evaluate_candidate_on_task(self, task, candidate_str):\n",
        "      \"\"\"\n",
        "      Returns a dict with:\n",
        "      {\n",
        "          \"code\": candidate_str,\n",
        "          \"evaluation\": [outputs for train grids..., outputs for test grids...]\n",
        "      }\n",
        "      \"\"\"\n",
        "      data = task.to_dict()\n",
        "      train_pairs = [(pair[\"input\"], pair[\"output\"]) for pair in data[\"train\"]]\n",
        "      test_inputs = [pair[\"input\"] for pair in data[\"test\"]]\n",
        "\n",
        "      evaluation_results = []\n",
        "\n",
        "      # Evaluate on train grids\n",
        "      for input_grid, _ in train_pairs:\n",
        "          output = self.run_candidate_func(candidate_str, input_grid)\n",
        "          evaluation_results.append(output if output is not None else \"ERROR\")\n",
        "\n",
        "      # Evaluate on test grids\n",
        "      for input_grid in test_inputs:\n",
        "          output = self.run_candidate_func(candidate_str, input_grid)\n",
        "          evaluation_results.append(output if output is not None else \"ERROR\")\n",
        "\n",
        "      return {\"code\": candidate_str, \"evaluation\": evaluation_results}\n",
        "\n",
        "\n",
        "\n",
        "  #build refinement prompt\n",
        "  def build_arc_repair_prompt_from_arckit(self, task, candidate: dict):\n",
        "      \"\"\"\n",
        "      Build an ARC repair-style prompt from an arckit.Task object\n",
        "      and a single candidate implementation.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      task : arckit.Task\n",
        "      candidate : dict\n",
        "          {\n",
        "              \"code\": str,\n",
        "              \"evaluation\": list[str]   # ordered: train → test\n",
        "          }\n",
        "      \"\"\"\n",
        "\n",
        "      data = task.to_dict()\n",
        "\n",
        "      train_pairs = [(pair[\"input\"], pair[\"output\"]) for pair in data[\"train\"]]\n",
        "      test_inputs = [pair[\"input\"] for pair in data[\"test\"]]\n",
        "\n",
        "      header = \"\"\"\n",
        "  You are an AI assistant specialized in solving Abstract Reasoning Corpus (ARC-AGI) tasks by\n",
        "  repairing Python code implementations.\n",
        "  Your goal is to analyze input-output grid pairs. The outputs were produced by applying a\n",
        "  transformation rule to the inputs.\n",
        "  You will be given a python function `transform` that was supposed to implement the\n",
        "  transformation rule, but it is not working correctly for all inputs.\n",
        "  Your role is to fix this `transform` function.\n",
        "\n",
        "  Your solution should be:\n",
        "  - Accurate: Correctly fix the transformation for all given inputs\n",
        "  - Comprehensive: Handles all possible input scenarios\n",
        "  - Well-structured: Uses clear, readable, and efficient code\n",
        "\n",
        "  The number in the input grid can be mapped to the following colors:\n",
        "  0:Black; 1:Blue; 2:Red; 3:Green; 4:Yellow; 5:Grey;\n",
        "  6:Pink; 7:Orange; 8:Purple; 9:Brown\n",
        "\n",
        "  Now, repair the following ARC-AGI task implementation:\n",
        "  \"\"\"\n",
        "\n",
        "      def format_grid(grid):\n",
        "          rows = [\"[\" + \" \".join(str(x) for x in row) + \"]\" for row in grid]\n",
        "          return \"[\" + \"\\n\".join(rows) + \"]\"\n",
        "\n",
        "      parts = []\n",
        "\n",
        "      for i, (inp_grid, out_grid) in enumerate(train_pairs, start=1):\n",
        "          parts.append(\n",
        "  f\"\"\"\n",
        "  ## Input {i} (grid shape: {len(inp_grid)} by {len(inp_grid[0])}):\n",
        "  {format_grid(inp_grid)}\n",
        "\n",
        "  ## Output {i} (grid shape: {len(out_grid)} by {len(out_grid[0])}):\n",
        "  {format_grid(out_grid)}\n",
        "  \"\"\"\n",
        "          )\n",
        "\n",
        "      for j, inp_grid in enumerate(test_inputs, start=1):\n",
        "          parts.append(\n",
        "  f\"\"\"\n",
        "  ## Test Input {j} (grid shape: {len(inp_grid)} by {len(inp_grid[0])}):\n",
        "  {format_grid(inp_grid)}\n",
        "  \"\"\"\n",
        "          )\n",
        "\n",
        "      evaluation_blocks = \"\"\n",
        "      for k, result in enumerate(candidate[\"evaluation\"], start=1):\n",
        "          evaluation_blocks += f\"\"\"\n",
        "  ## Evaluation Result {k}\n",
        "  {result}\n",
        "  \"\"\"\n",
        "\n",
        "      candidate_section = f\"\"\"\n",
        "  Previous implementation:\n",
        "  ```python\n",
        "  {candidate[\"code\"].strip()}\n",
        "  ```\n",
        "\n",
        "  This implementation produced the following results:\n",
        "  {evaluation_blocks}\n",
        "\n",
        "  The above implementation produced incorrect results.\n",
        "\n",
        "  Fix the transform function so that it produces correct outputs for all training examples\n",
        "  and generalizes to the test inputs.\n",
        "\n",
        "  You must return ONLY the corrected Python code in triple backticks.\n",
        "  \"\"\"\n",
        "\n",
        "      return header + \"\\n\".join(parts) + candidate_section\n",
        "\n",
        "\n",
        "    #build crossover prompt from two python function strings\n",
        "  def build_crossover_prompt(self, func_a_str, func_b_str):\n",
        "    \"\"\"\n",
        "    Returns a prompt string for performing GP crossover between two Python functions.\n",
        "    func_a_str, func_b_str: Python functions as strings\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "\n",
        "You are performing genetic programming crossover on Python code.\n",
        "\n",
        "You are given two Python functions.\n",
        "\n",
        "STEP 1 — STANDARDISATION (MANDATORY):\n",
        "- Rename variables so BOTH functions use:\n",
        "  - n for grid size\n",
        "  - output for the result grid\n",
        "- Do NOT introduce new variables.\n",
        "- Only rename existing variables.\n",
        "- The standardized versions MUST be different from at least one original.\n",
        "\n",
        "STEP 2 — CROSSOVER (MANDATORY):\n",
        "- Create exactly TWO child functions.\n",
        "- Each child MUST:\n",
        "  - Contain at least ONE line originally from Function A\n",
        "  - Contain at least ONE line originally from Function B\n",
        "- Returning either parent unchanged is FORBIDDEN.\n",
        "- Renaming alone does NOT count as crossover.\n",
        "\n",
        "RULES:\n",
        "- Use ONLY lines that appear in the standardized functions.\n",
        "- Do NOT invent new lines.\n",
        "- Do NOT introduce new variables.\n",
        "- Do NOT explain anything.\n",
        "- Each child must be enclosed in back ticks\n",
        "      Output Child 1:\n",
        "      ```python\n",
        "      # Your first child function here\n",
        "      ```\n",
        "\n",
        "      Output Child 2:\n",
        "      ```python\n",
        "      # Your second child function here\n",
        "      ```\n",
        "- Output ONLY valid Python code.\n",
        "- All children MUST use the function name: transform\n",
        "\n",
        "Function A:\n",
        "```python\n",
        "{func_a_str}\n",
        "```\n",
        "\n",
        "Function B:\n",
        "```python\n",
        "{func_b_str}\n",
        "```\n",
        "\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "\n",
        "  def build_mutation_prompt(self, func_str):\n",
        "    \"\"\"\n",
        "    Returns a prompt string for performing GP mutation on a Python function.\n",
        "    func_str: Python function as a string\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are a genetic programming mutation operator.\n",
        "\n",
        "You are given a Python function that transforms a grid (2D list of integers).\n",
        "\n",
        "Generate EXACTLY 3 mutated child functions by applying ONE small structural mutation per child.\n",
        "\n",
        "RULES (MANDATORY):\n",
        "\n",
        "1. All children must keep the function signature EXACTLY:\n",
        "   def transform(grid):\n",
        "\n",
        "2. Keep the grid size definition line unchanged (e.g., n = len(grid)).\n",
        "\n",
        "3. Preserve output dimensions and type.\n",
        "\n",
        "4. Use ONLY the following primitives:\n",
        "   - Variables: grid, output, i, j, n\n",
        "   - Integers: 0, 1\n",
        "   - Operators: +, -, *\n",
        "   - Indexing: grid[...][...], output[...][...]\n",
        "   - Loops: for i in range(n), for j in range(n)\n",
        "   - Assignment: output[a][b] = grid[c][d]\n",
        "\n",
        "5. Do NOT introduce:\n",
        "   - New variables\n",
        "   - New functions\n",
        "   - Conditionals\n",
        "   - Function calls\n",
        "   - Additional loops\n",
        "\n",
        "6. Each child must differ from the original by EXACTLY ONE line of logic.\n",
        "\n",
        "7. Each child must differ from the other children.\n",
        "\n",
        "8. Remove any dead or redundant code.\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "Each mutant must be enclosed in separate code blocks:\n",
        "```python\n",
        "# Mutant 1\n",
        "```\n",
        "```python\n",
        "# Mutant 2\n",
        "```\n",
        "```python\n",
        "# Mutant 3\n",
        "```\n",
        "\n",
        "Original function:\n",
        "```python\n",
        "{func_str}\n",
        "```\n",
        "\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n"
      ],
      "metadata": {
        "id": "9hIv77EoOnAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_helper = ARCTasksHelper()\n",
        "train_set_prompts, test_set_prompts = task_helper.generate_task_dataset_prompts()"
      ],
      "metadata": {
        "id": "jXGsjr8aXh2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KP2aHlcyb5Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_set_prompts[0])"
      ],
      "metadata": {
        "id": "yXMIx1CwYAoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Outer loop\n",
        "- Take in prompts to\n",
        "      1. Either generate initial population\n",
        "      2. Or Refine solution and generate better population\n",
        "-\n",
        "\"\"\"\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Model(ABC):\n",
        "\n",
        "  #base LLM, owns model and tokenizer\n",
        "  # Class-level shared model and tokenizer\n",
        "  model = None\n",
        "  tokenizer = None\n",
        "\n",
        "  def __init__(self, model_name):\n",
        "      if Model.model is None or Model.tokenizer is None:\n",
        "        self.model_name = model_name\n",
        "        self._load_model()\n",
        "      else:\n",
        "            print(\"Using shared model instance\")\n",
        "\n",
        "  def _load_model(self):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "    #4-bit quantization\n",
        "    bnb_config_4bit = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "\n",
        "\n",
        "    model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "        self.model_name,\n",
        "        quantization_config=bnb_config_4bit,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.model = model_4bit\n",
        "\n",
        "    print(\"Model Loaded Succeffully\")\n",
        "\n",
        "\n",
        "  #Get model response\n",
        "  @abstractmethod\n",
        "  def get_model_response(self, prompt):\n",
        "    pass\n",
        "\n",
        "  #extract python function can also be done here\n",
        "\n",
        "  def extract_python_function(self, text):\n",
        "      \"\"\"\n",
        "      Extracts the first Python function from text (from 'def' to 'return').\n",
        "      Returns a single function string or None if not found.\n",
        "      \"\"\"\n",
        "      # Match code blocks with various language tags\n",
        "      pattern = r\"```(?:python|py|notebook-python)?\\s*(.*?)```\"\n",
        "      blocks = re.findall(pattern, text, flags=re.DOTALL)\n",
        "\n",
        "      # If code blocks found, use them; otherwise use raw text\n",
        "      code_text = \"\\n\\n\".join(blocks) if blocks else text\n",
        "\n",
        "      # Extract first function from 'def' to 'return' statement\n",
        "      func_pattern = r\"(def\\s+\\w+\\s*\\([^)]*\\):.*?return\\s+[^\\n]+)\"\n",
        "      match = re.search(func_pattern, code_text, flags=re.DOTALL)\n",
        "\n",
        "      if match:\n",
        "          return match.group(1).strip()\n",
        "\n",
        "      # Fallback: if no return statement, try to get function with indentation logic\n",
        "      func_pattern_no_return = r\"(def\\s+\\w+\\s*\\([^)]*\\):(?:.*?)(?=\\ndef\\s+\\w+|\\Z))\"\n",
        "      match = re.search(func_pattern_no_return, code_text, flags=re.DOTALL)\n",
        "\n",
        "      if match:\n",
        "          return match.group(1).strip()\n",
        "\n",
        "      return None\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FSdZLibSN3O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scientist, (separated from engineer to make it easier to adopt different models for the diff purposes is need be)\n",
        "class Scientist(Model):\n",
        "\n",
        "  #the input prompt, can either be for seeding, or for refining\n",
        "  def get_model_response(self, prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI assistant specialized in solving Abstract Reasoning Corpus (ARC-AGI) tasks by reasoning and generating Python code.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    text = self.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "\n",
        "    generated_ids = self.model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample = True,\n",
        "        temperature=1.0,   # SOAR setting\n",
        "        min_p=0.05,\n",
        "    )\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    return response\n",
        "\n",
        "    #create a population, by sampling the llm n times\n",
        "  def get_population(self, pop_size, prompt):\n",
        "    population = []\n",
        "\n",
        "    for _ in range(pop_size):\n",
        "        candidate = self.get_model_response(prompt)\n",
        "        extracted_candidate = self.extract_python_function(candidate)\n",
        "\n",
        "        if extracted_candidate is not None:\n",
        "            population.append(extracted_candidate)\n",
        "\n",
        "    return population\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H_cIuuTp_g03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Engineer\n",
        "class Engineer(Model):\n",
        "\n",
        "  #the input prompt, can either be for seeding, or for refining\n",
        "  def get_model_response(self, prompt, type = None):    #type can either be mutator or cross-over\n",
        "\n",
        "    # 1. Define Messages based on type\n",
        "\n",
        "    if type == \"cross-over\":\n",
        "      messages = [\n",
        "          {\"role\": \"system\", \"content\": \"You are a genetic programming crossover operator. You recombine Python code by copying mandatory fragments from multiple parents\"},\n",
        "          {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "\n",
        "\n",
        "\n",
        "    elif type == \"mutator\":\n",
        "      messages = [\n",
        "          {\"role\": \"system\", \"content\": \"You are an AI assistant specialized in creating structural variations of Python functions without knowing their purpose by reasoning and generating Python code\"},\n",
        "          {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "\n",
        "\n",
        "\n",
        "    else:\n",
        "      print (\"Type not recognized !!\")\n",
        "      return \"\"\n",
        "\n",
        "    text = self.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "    if type == 'cross-over':\n",
        "      generated_ids = self.model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample = False,\n",
        "      )\n",
        "\n",
        "    elif type == 'mutator':\n",
        "      generated_ids = self.model.generate(\n",
        "          **model_inputs,\n",
        "          max_new_tokens=1024,\n",
        "          do_sample = True,\n",
        "          top_p=0.9,\n",
        "          temperature=1.2,\n",
        "          #repetition_penalty=1.0\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    return response\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4Na-EZbjBrnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evolutionary Loop"
      ],
      "metadata": {
        "id": "PeS-YJS8gfls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class EvolutionaryOptimizer:\n",
        "    def __init__(self, task_helper, scientist, engineer):\n",
        "        self.helper = task_helper\n",
        "        self.scientist = scientist\n",
        "        self.engineer = engineer\n",
        "\n",
        "        # GP Hyperparameters\n",
        "        self.POPULATION_SIZE = 10\n",
        "        self.MAX_GENERATIONS = 5\n",
        "        self.ELITISM_COUNT = 2       # Keep the top 2 best codes\n",
        "        self.CROSSOVER_RATE = 0.6    # 60% probability\n",
        "        self.MUTATION_RATE = 0.4     # 40% probability\n",
        "\n",
        "    def evaluate_population(self, population, task):\n",
        "        \"\"\"\n",
        "        Runs every candidate against the task's training pairs.\n",
        "        Returns sorted list of (score, function_code).\n",
        "        \"\"\"\n",
        "        data = task.to_dict()\n",
        "        train_pairs = [(pair[\"input\"], pair[\"output\"]) for pair in data[\"train\"]]\n",
        "\n",
        "        scored_population = []\n",
        "        for code in population:\n",
        "            # Uses your score_candidate_on_train_pair method\n",
        "            score = self.helper.score_candidate_on_train_pair(code, train_pairs)\n",
        "            scored_population.append((score, code))\n",
        "\n",
        "        # Sort descending (Higher score is better)\n",
        "        scored_population.sort(key=lambda x: x[0], reverse=True)\n",
        "        return scored_population\n",
        "\n",
        "    def tournament_selection(self, scored_population, k=3):\n",
        "        \"\"\"Picks a parent using tournament selection.\"\"\"\n",
        "        # Sample k individuals\n",
        "        tournament = random.sample(scored_population, min(k, len(scored_population)))\n",
        "        # Return the code of the one with the highest score\n",
        "        return max(tournament, key=lambda x: x[0])[1]\n",
        "\n",
        "    def evolve(self, task, seed_population):\n",
        "        \"\"\"\n",
        "        Main Evolutionary Loop.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Starting Evolution ---\")\n",
        "        population = seed_population\n",
        "        best_solution = None\n",
        "        best_score = -1.0\n",
        "\n",
        "        for gen in range(self.MAX_GENERATIONS):\n",
        "            # 1. Evaluate\n",
        "            scored_pop = self.evaluate_population(population, task)\n",
        "\n",
        "            # Statistics\n",
        "            gen_best_score, gen_best_code = scored_pop[0]\n",
        "            print(f\"Generation {gen+1}: Best Score = {gen_best_score:.2f}\")\n",
        "\n",
        "            # Check for Perfect Solution\n",
        "            if gen_best_score >= 1.0:\n",
        "                print(\">>> Perfect Solution Found!\")\n",
        "                return gen_best_code\n",
        "\n",
        "            # Update global best\n",
        "            if gen_best_score > best_score:\n",
        "                best_score = gen_best_score\n",
        "                best_solution = gen_best_code\n",
        "\n",
        "            # 2. Elitism (Carry over best individuals)\n",
        "            next_generation = [code for _, code in scored_pop[:self.ELITISM_COUNT]]\n",
        "\n",
        "            # 3. Breeding\n",
        "            while len(next_generation) < self.POPULATION_SIZE:\n",
        "\n",
        "                # Roll dice for Crossover vs Mutation\n",
        "                if random.random() < self.CROSSOVER_RATE:\n",
        "                    # --- CROSSOVER ---\n",
        "                    parent_a = self.tournament_selection(scored_pop)\n",
        "                    parent_b = self.tournament_selection(scored_pop)\n",
        "\n",
        "                    # Try to get a different parent B if possible\n",
        "                    if parent_a == parent_b and len(scored_pop) > 1:\n",
        "                        parent_b = self.tournament_selection(scored_pop)\n",
        "\n",
        "                    prompt = self.helper.build_crossover_prompt(parent_a, parent_b)\n",
        "\n",
        "                    # Engineer generates 2 children\n",
        "                    response = self.engineer.get_model_response(prompt, type=\"cross-over\")\n",
        "\n",
        "                    # Extract using the NEW helper method that supports multiple functions\n",
        "                    children = self.helper.extract_python_functions(response)\n",
        "\n",
        "                    if children:\n",
        "                        # Randomly sample one child for diversity\n",
        "                        selected_child = random.choice(children)\n",
        "                        next_generation.append(selected_child)\n",
        "                    else:\n",
        "                        # Fallback: keep parent if generation failed\n",
        "                        next_generation.append(parent_a)\n",
        "\n",
        "                else:\n",
        "                    # --- MUTATION ---\n",
        "                    parent = self.tournament_selection(scored_pop)\n",
        "                    prompt = self.helper.build_mutation_prompt(parent)\n",
        "\n",
        "                    # Engineer generates mutants\n",
        "                    response = self.engineer.get_model_response(prompt, type=\"mutator\")\n",
        "\n",
        "                    # Extract mutants\n",
        "                    mutants = self.helper.extract_python_functions(response)\n",
        "\n",
        "                    if mutants:\n",
        "                        # Randomly sample one child for diversity\n",
        "                        selected_mutant = random.choice(mutants)\n",
        "                        next_generation.append(selected_mutant)\n",
        "                    else:\n",
        "                        next_generation.append(parent)\n",
        "\n",
        "            # Ensure population size stays constant (truncate if crossover added too many)\n",
        "            population = next_generation[:self.POPULATION_SIZE]\n",
        "\n",
        "        print(f\"Evolution finished. Best Score: {best_score}\")\n",
        "        return best_solution\n"
      ],
      "metadata": {
        "id": "yCkCUKy9gfYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize\n",
        "helper = ARCTasksHelper()\n",
        "\n",
        "# Load the base model once\n",
        "base_model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "base_model = Scientist(base_model_name)  # use Scientist just to load the model, Now base_model.model and base_model.tokenizer are available\n",
        "\n",
        "# Create Scientist and Engineer pointing to the same model/tokenizer\n",
        "scientist = Scientist.__new__(Scientist)  # skip __init__\n",
        "scientist.model = base_model.model\n",
        "scientist.tokenizer = base_model.tokenizer\n",
        "\n",
        "engineer = Engineer.__new__(Engineer)      # skip __init__\n",
        "engineer.model = base_model.model\n",
        "engineer.tokenizer = base_model.tokenizer\n",
        "optimizer = EvolutionaryOptimizer(helper, scientist, engineer)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0LWk5zXjGCr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load Prompts\n",
        "train_prompts, test_prompts = helper.generate_task_dataset_prompts()\n",
        "\n",
        "# 3. Select Task (e.g., index 0)\n",
        "task_idx = 0\n",
        "task_obj = helper.train_set[task_idx]\n",
        "seed_prompt = train_prompts[task_idx]\n",
        "\n",
        "print(f\"Solving Task {task_idx}...\")\n",
        "\n",
        "# 4. Phase 1: Scientist Seeding\n",
        "print(\"--- Phase 1: Seeding ---\")\n",
        "seed_population = scientist.get_population(pop_size=5, prompt=seed_prompt)\n",
        "\n",
        "if seed_population:\n",
        "    # 5. Phase 2: Evolution\n",
        "    best_code = optimizer.evolve(task_obj, seed_population)\n",
        "\n",
        "    print(\"\\n=== Final Evolved Solution ===\")\n",
        "    print(best_code)\n",
        "else:\n",
        "    print(\"Scientist failed to generate seeds.\")"
      ],
      "metadata": {
        "id": "6vj8MGZ4AbJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_V06Uyk-EKGy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}